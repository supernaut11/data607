---
title: "DATA607 - Week 10 Homework"
author: "Mike Dehn"
date: "11/17/2021"
output:
  pdf_document:
    latex_engine : pdflatex
    number_sections: true
bibliography: sources.bib
---

```{r, setup, include=FALSE}
library("caret")
library("curl")
library("randomForest")
library("ROCR")
library("tidyverse")
```

# Introduction

Determining which mushrooms are edible versus poisonous is an important 
survival skill for those who find themselves searching for food in the 
wilderness. In this study, we build logistic regression and random forest
models that assess the edibility of various mushroom species based on several
key characteristics. Both models have high classification performance, with
logistic regression slightly outperforming random forest.

# The Mushroom Data Set

The Mushroom Data Set is a collection of mushroom characteristics originally
described in The Audubon Society Field Guide to North American Mushrooms
[@audobon_guide]. These records contain hypothetical samples of over 8,000 
individual mushrooms, and include features such as the mushroom shape, color,
and habitat. All features in the data set are categorical. Each mushroom is
labeled as either "edible" or "poisonous".

The data set used in this study is derived from the Machine Learning Repository
maintained by the University of California, Irvine [@Dua:2019]. The data set 
is directly downloaded as part of this paper from an Amazon Web Services (AWS)
S3 bucket [@mushroom_data].

## Acquiring Data

We download the mushroom data set from the AWS S3 bucket using the `curl`
R package. The data are stored in a Comma-Separated Value (CSV) file with
comma delimiters. Thus, the data are easily ingested using the `read_csv`
function provided by the `tidyverse` package.

```{r, message=FALSE}
connection <- curl::curl(
    "https://s3.amazonaws.com/notredame.analytics.data/mushrooms.csv")
raw_data <- read_csv(connection)
```

## Cleaning and Transforming the Data

We process the raw data retrieved from the S3 bucket using a pipeline of data
transformation operations.

First, we downselect our data to only include the original labels (i.e., 
`type`) and four specific features: `odor`, `cap_shape`, `cap_color`, and
`gill_attachment`. All other fields are excluded from our analysis and will
therefore have no impact on the outcome of our classifiers.

Each field in the mushroom data set is categorical, meaning R represents them 
as character strings by default. We transform the columns into factors so that
modeling algorithms can ingest the data set without raising errors.

The label field, `type`, is currently stored as a non-binary factor, which is
problematic for the R implementation of the logistic regression algorithm. We
add a step to the pipeline which converts the `type` field from a factor to an
integer. This transforms "edible" to 1, and "poisonous" to 2. To create a
proper binary field (i.e., 0/1-valued), we subtract 1 from this column and then
transform it back into a factor. Both the linear regression and random forest
models can correctly interpret the `type` label when it is stored as a 
0/1-valued factor.

The final step of our pipeline filters out all samples that contain an `NA` 
value in any of its columns. Although the current data set does not include any
samples with `NA` values, the inclusion of this step shields us from future
updates to the data set that might include an `NA` value.

The R pipeline below performs each of the operations described in this section.

```{r}
# Transform raw data into factorized columns. Also change type column into
# binary field where 0 = edible; 1 = poisonous
mushrooms <- raw_data %>%
    select(type, odor, cap_shape, cap_color, gill_attachment) %>%
    mutate_all(as.factor) %>%
    mutate(type = as.factor(as.numeric(type) - 1)) %>%
    na.exclude
```

Upon execution of this code, the mushroom data set is usable for both linear
regression and random forest classifiers.

## Establishing Training vs. Test Data

Splitting the mushroom data set into separate training and test sets allows us
to build and train models independently of the data used to validate their
performance. Without a train/test split, we have less confidence that our model
generalizes to data it has not seen. For the purposes of this analysis, we 
divide the mushroom data set such that 70 percent of our samples are the
training set, and the remaining 30 percent are the test set.

Prior to splitting the data set, we assign a static random seed via the
`set.seed` function. This makes our results reproducible across multiple
executions by seeding R's random number generator with a known state before
performing operations that require randomness (e.g., randomly sampling the 
whole data set for training data).

```{r}
# Set up training data with a 70/30 split
set.seed(1)
num_train <- floor(nrow(mushrooms) * 0.7)
train_idx <- sample(nrow(mushrooms), num_train)
train_data <- mushrooms[train_idx, ]
test_data <- mushrooms[-train_idx, ]
```

The mushroom data set is now ingested, transformed, and prepared for analysis.
In the proceeding section, we build classifiers that label mushrooms as either
"edible" or "poisonous".

# Edibility Classifier Models

In this analysis, we explore two models that classify the edibility of 
mushrooms using logistic regression and random forest algorithms. These models
are built with basic implementations provided in R packages.

## Logistic Regression Classifier

The logistic regression classifier is a model that uses the logistic function
to represent the probability that an event occurred. By using a maximum 
likelihood estimation approach, we build a binary classifier where the label
is assigned using a cutoff at $p(event) = 0.5$. In other words, a label, $y$ is
assigned such that:

$$
y = 
\begin{cases}
  0 \text{ if } p(event) \leq 0.5 \\
  1 \text{ otherwise}
\end{cases}
$$

Using this rule, we assign a label to a record based on which label is the most
likely in the context of the logistic regression model. In the case of the 
mushroom data set, we represent an edible mushroom as a non-event (valued 0),
and a poisonous mushroom as an event (valued 1).

### Building the Model

The `glm` function provided by R contains an implementation of the logistic
regression algorithm. We provide the mushroom data that we ingested and cleaned
earlier in this report to the `glm` function, providing it only with our
training data set. The logistic regression classifier is configured to predict
the `type` label for the mushroom, which was converted to a binary value in our
previous transformations. Ultimately, the model provides the probability that
a mushroom is poisonous.

The code snippet below trains our logistic regression model for the mushroom
data set.

```{r, include=FALSE}
Rprof()
```
```{r, message=FALSE}
# Build logistic regression model and collect profiling info
lr_model <- glm(type ~ ., data = train_data, family = "binomial")
```
```{r, include=FALSE}
Rprof(NULL)
lr_train_time <- summaryRprof()$by.total$total.time[1]
```

The variable `lr_model` now contains our logistic regression model which we use
to classify mushrooms.

### Making Predictions

The `lr_model` logistic regression model we built is now able to perform 
classification of mushrooms on our test data set. To exercise the model, we 
use the `predict` function provided by R. Since our logistic regression model
is designed to provide $p(poisonous)$, we output a label of "0" (i.e., edible)
if $p(poisonous) < 0.5$, otherwise, we output a label of "1" (i.e., poisonous).

```{r}
# Make predictions with logisitic regression model
lr_pred_probs <- predict(lr_model, test_data, type = "response")
lr_pred <- ifelse(lr_pred_probs < 0.5, "0", "1")
```

The confusion matrix below represents the results of our classification against
the test data set.

```{r, echo=FALSE}
confusionMatrix(table(lr_pred, mushrooms$type[-train_idx]), positive = "1")
```

The ROC curve below provides a graphical representation of the classifier's
performance.

```{r, echo=FALSE}
lr_pred_roc <- prediction(lr_pred_probs, mushrooms$type[-train_idx])
roc <- performance(lr_pred_roc, "tpr", "fpr")
plot(roc, colorize = FALSE, lwd = 2, col = "blue", main = "ROC curve")
lines(c(0:100) / 100, c(0:100) / 100, lty = 2)
auc <- performance(lr_pred_roc, measure = "auc")
auc <- auc@y.values[[1]]
text(0.85, 0.1, "AUC=", cex = 1)
text(0.95, 0.1, round(auc, 3), cex = 1)
```

## Random Forest

The random forest classifier is a model that uses an ensemble of decision trees
to classify data records based on comparison of features against. Each node in
a decision tree represents a feature to evaluate against a boundary condition,
and each edge represents a transition from one decision state to another. Leaf
nodes in the tree represent a final decision and result in a classification
decision.

A random forest consists of $n$ decision trees trained against the same data
set. It is an ensemble classifier, meaning that the ultimate classification
decision is made based on votes from the constituent decision tree models. A
random forest very often outperforms decision trees in classification tasks, so
we use this algorithm instead of a decision tree for classifying mushrooms.

One major benefit of decision tree and random forest classifiers is that it is
straightforward to determine and represent the most important features in a
model. Specifically, we can use the Gini importance measure which describes the
mean decrease in impurity after evaluating a feature. A higher Gini importance
can be thought of as higher predictive power of a feature.

### Building the Model

The `randomForest` function provided by R contains an implementation of the 
random forest algorithm. We provide the mushroom data that we ingested and
cleaned earlier in this report to the `randomForest` function, providing it 
only with our training data set. The random forest classifier is configured
to predict the `type` label for the mushroom, which was converted to a binary
value in our previous transformations. Ultimately, the model provides a
decision as to whether a mushroom is poisonous or edible.

The code snippet below trains our random forest model for the mushroom data set.

```{r, include=FALSE}
Rprof()
```
```{r}
# Build random forest model
rf_model <- randomForest(type ~ ., data = train_data)
```
```{r, include=FALSE}
Rprof(NULL)
rf_train_time <- summaryRprof()$by.total$total.time[1]
```

### Making Predictions

The random forest model we built is now able to perform classification of
mushrooms on our test data set. To exercise the model, we use the `predict`
function provided by R. Since our random forest model is designed to provide
decisions, we do not have to perform a translation of the model's output, we
can use it as-is. For the output of the random forest model, a label of "0"
indicates the mushroom is edible, while a label of "1" indicates it is
poisonous.

```{r}
# Make predictions with random forest model
rf_pred <- predict(rf_model, test_data, type = "class")
```

The confusion matrix below represents the results of our classification against
the test data set.

```{r, echo=FALSE}
confusionMatrix(table(rf_pred, mushrooms$type[-train_idx]), positive = "1")
```

The ROC curve below provides a graphical representation of the classifier's
performance.

```{r, echo=FALSE}
rf_pred_roc <- predict(rf_model, newdata = test_data, type = "prob")
rf_pred_roc <- prediction(rf_pred_roc[, 2],
                          mushrooms$type[-train_idx])
roc <- performance(rf_pred_roc, "tpr", "fpr")
plot(roc, colorize = FALSE, lwd = 2, col = "blue", main = "ROC curve")
lines(c(0:100) / 100, c(0:100) / 100, lty = 2)
auc <- performance(rf_pred_roc, measure = "auc")
auc <- auc@y.values[[1]]
text(0.85, 0.1, "AUC = ", cex = 1)
text(0.95, 0.1, round(auc, 3), cex = 1)
```

# Comparative Analysis

We evaluate the performance of the linear regression and random forest
classifiers with respect to their training time and performance in classifying
mushrooms correctly.

## Runtime

In training each of the models, we use the `Rprof` profiler to collect runtime
information. The runtime performance is summarized in the table below.

$$
{
\centering
\begin{tabular}{|c|c|}
  \hline
  \textbf{Model} & \textbf{Training Time (sec)} \\ \hline
  Logistic Regression & `r lr_train_time` \\ \hline
  Random Forest & `r rf_train_time` \\ \hline
\end{tabular}
}
$$

From the runtimes shown, we conclude that the linear regression classifier is
training time is significantly faster than that of the random forest classifier.
It is important to note that the random forest classifier training time is 
variable based on the number of decision trees in the forest. However, the
runtime difference between the logistic regression classifier
(`r lr_train_time` seconds) and the random forest classifier
(`r rf_train_time` seconds) is large enough that we can conclude the logistic
regression classifier is the superior choice in this respect.

## Classifier Performance

As shown in previous sections, the performance of the logistic regression and
random forest classifiers is very similar. To summarize some key performance
characteristics:

$$
{
\centering
\begin{tabular}{|c|c|c|}
  \hline
  \textbf{Model} & \textbf{Accuracy} & \textbf{Sensitivity} \\
  \hline
  Logistic Regression & 0.9897 & 0.9787 \\ \hline
  Random Forest & 0.9889 & 0.9770 \\ \hline
\end{tabular}
}
$$

As we can see, the results for both the accuracy and sensitivity are remarkably
similar across the logistic regression and random forest classifiers. However, 
the logistic regression classifier has higher accuracy and sensitivity.

This is further confirmed when viewing the ROC curves presented in previous 
sections. Both ROC curves for the classifiers have a very high area under the 
curve, 0.999 for logistic regression, and 0.995 for random forest.

# Conclusion

Given the above results, we conclude that the logistic regression classifier is
the superior model for classifying mushrooms as either edible or poisonous. The
logistic regression classifier has a much shorter training time (less than 1
second), while the random forest classifier requires much longer. 

Further, the logistic regression classifier has a higher accuracy and
sensitivity. In this context, higher sensitivity corresponds to fewer mistakes 
where mushrooms are misclassified as edible when they are actually poisonous.

Given these factors, we conclude that the logistic regression model is
preferable since it is both faster to train and safer to use.

# References
